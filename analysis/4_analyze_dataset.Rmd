---
title: "Analyze dataset"
date: "`r Sys.Date()`"
bibliography: references.bib
biblio-style: apalike
---

## About

### Description

The purpose of this script is to compare the two datasets, Love is Blind and Love on the Spectrum and find the differences between the two datasets and thus, the two groups of individuals (those on the Autism spectrum and those who are not). It is important to note that

We will start by doing a keyness contrast analysis by finding the relative frequency of words, then we will do a weighted frequency to determine how frequent a particular word is relative to the entire dialogue from the season. Next, we will look to determine whether or not there is lexical diversity between the two groups. Then, we will do a type-token analysis. Finally we will do topic modeling to show differences amongst different types of people and then the last step we plan to do is a sentiment analysis in order to find if there are different emotions that are expressed in the two different seasons.

It is important to note that though the two shows are about navigating the dating world, we do know that Love on the Spectrum and Love is Blind have different motives. Love on the Spectrum is more about following individuals around as they navigate looking and falling in love, whereas Love is Blind is more of a show based on competition and may be more scripted and less natural.

### Usage

<!-- How to run this script: what input it requires and output produced -->

## Setup

```{r setup}
# Script-specific options or packages

library(tidyverse)           # data manipulation
library(patchwork)           # organize plots
library(janitor)             # cross tabulations
library(tidytext)            # text operations
library(quanteda)            # tokenization and document-frequency matrices
library(quanteda.textstats)  # descriptive text statistics
library(quanteda.textmodels) # topic modeling
library(quanteda.textplots)  # plotting quanteda objects
```

## Run

In our transform stage, we combined the two datasets into one. Let's read in this combined datset.

```{r read-in-dataset, message=FALSE}
love <- read_csv(file = "../data/derived/love.csv")
```

Here, we see this is our new (combined) dataset because it has 10 rows.

```{r}
glimpse(love) # preview dataset 
```

Let's take a look at the data dictionary to make sure we understand what each variable signifies (our variables are very straight forward, but I recommend this step regardless to make sure you know what each variable represents). We will only preview one data dictionary because both contain the same variables: Series, Season, Episode, and Dialogue.

```{r}
read_csv(file = "../data/derived/love_on_the_spectrum/love_on_the_spectrum_curated_data_dictionary.csv") # read data dictionary
```

### Word Frequency Analysis

```{r create-corpus-object}
love_corpus <-
  love %>% # data frame
  corpus(text_field = "Dialogue")

love_corpus %>%
  summary(n = 5) # preview
```

This code chunk allow us to see the type-tokens for each episode in the seasons for Love on the Spectrum and Love is Blind.

```{r create-tokens-object}
love_tokens <-
  love_corpus %>% # corpus object
  tokens(what = "word", # tokenize by word
         remove_punct = TRUE) %>% # remove punctuation
  tokens_tolower() # lowercase tokens


love_tokens %>%
  head(n = 1) # preview one tokenized documents.
```

This shows us each of the tokens, which we organize by word, of each series.

```{r create-document-frequency-matrix}
love_dfm <-
  love_tokens %>%
  dfm() # create dfm

love_dfm %>% # data frequency model for love on the spectrum
  head(n = 5) # preview 5 documents
```

This data frequency model shows the first 10 words of the transcripts from 5 of the 10 episodes (docs). Under each word, it shows its frequency in each of the 5 episodes we are previewing. We see there is a 75.3% sparsity, which indicates there are a number of zeros throughout the dataset for the features. There are 4,122 features total in the 5 documents.

We are now going to look at the top 25 words from the combined dataset.

```{r most-frequent-words}
love_dfm %>%
  textstat_frequency() %>%
  slice_head(n = 10)
```

This list encompasses the top 10 words from each of the documents (episodes). We see that these words are common words that are used in everyday language use.

We are now going to use this data frame to plot the frequencies of the terms in descending order for the dataset (the top 25 words).

```{r dfm-ggplots}
love_dfm %>%
  textstat_frequency() %>%
  slice_head(n = 25) %>%
  ggplot(aes(x = reorder(feature, frequency), y = frequency)) + geom_col() + coord_flip() + labs(x = "Words", y = "Raw Frequency", title = "Top 25 Most Frequent Words: Both Series")
```

The raw frequency of each word in each dataset is affected by the total number of words in each series dataset. In order to make a term-series comparison, we will now use the dfm_weight() function which will determine the Term (weighted) frequency of the words by determining how frequent a term in an episode is to the rest of the episode.

```{r weighted-term-frequency}
love_dfm %>% # dfm
  dfm_weight(scheme = "prop") %>% # weigh by term frequency
  textstat_frequency() %>%
  group_by(group) %>% #grouping parameters
  slice_max(frequency, n = 15) %>% # extract top features
  ungroup() %>% # remove grouping parameters
  ggplot(aes(x = frequency, y = reorder_within(feature, frequency, group), fill = group)) +
  geom_col (show.legend = FALSE) + # barplot
  scale_y_reordered() + # clean up y-axis labels (features)
  labs(x = "Term Frequency", y = NULL) # labels
```

We see the use of the words "I" and "You" are used very frequently, much more than the rest of the words.

The issue we now have is that the words in the term frequency matrix is that the top 15 words appear to be terms that are the bread and butter across language-- they are extremely common. In order to find the words that distinguish (if they do) one series from another, we must apply the Term Frequency-Inverse Document Frequency to put the most common words at the bottom of the chart because they are most frequent across all the documents, and put the next set of words that aren't as common as the most frequent.

```{r tf-idf}
love_dfm%>%
  dfm_trim(min_docfreq = 2) %>% # keep terms appearing in 2 or more episodes
  dfm_tfidf(scheme_tf = "prop") %>% # weigh by tf-idf
  textstat_frequency(force = TRUE) %>% # get frequency statistics
  group_by(group) %>% #grouping parameters
  slice_max(frequency, n = 15) %>% # extract top features
  ungroup() %>% # remove grouping parameters
  ggplot(aes(x = frequency, y = reorder_within(feature, frequency, group), fill = group)) +
  geom_col (show.legend = FALSE) + # barplot
  scale_y_reordered() + # clean up y-axis labels (features)
  labs(x = "Term Frequency", y = NULL) # labels
```

After minimizing the effect of common words, we are now able to see the "meat" of the data and which words are used most frequently across the episodes. We see there are a lot of names that appear. We see no words relating to romantic relationships, but we do see explicits as well as references to autism and the spectrum.

Since the shows are dating shows and both reality shows in a sense, it makes sense that names are very common throughout the shows. However, this is distracting and doesn't allow us to really analyze whether we see patterns in the most frequent words.

Let's look at loading in and using the `babynames` dataset in order to eliminate the names we see listed above and look at the true content of the dataset.

```{r a-love-remove-names}
first_names <- 
  babynames::babynames %>% # babynames data frame
  distinct(name) %>% # get unique names
  pull(name) %>% # pull names into character vector
  str_to_lower() # lowercase all names

love_content_tokens <- 
  love_tokens %>% # original tokens object
  tokens_remove(first_names) # remove names in `first_names`

love_content_dfm <- 
  love_content_tokens %>% 
  dfm()

love_content_dfm %>% 
  dfm_tfidf() %>% 
  dfm_trim(min_docfreq = 2) %>% 
  textstat_frequency(groups = Series, force = TRUE) %>% 
  group_by(group) %>% 
  slice_max(frequency, n = 25) %>% 
  ungroup() %>% 
  ggplot(aes(x = frequency, y = reorder_within(feature, frequency, group), fill = group)) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  facet_wrap(~group, scales = "free_y") +
  labs(x = "Raw Frequency", y = NULL)
```

After eliminating all the names that took appeared in the top 15 word frequency, we are finally able to look at the meat of the dataset. We are also able to compare the raw frequency of the top 25 words of both Series. We see for the Love is Blind series, we see explicits are said quite frequently, but we also see words that reference love and romantic relationships like engaged, husband, wife, propose, fianc√©, wedded, and kids.

For the Love on the Spectrum series, there are a lot of references to Austism (autistic, autism, spectrum, asperger's, disability) and words that are "nerdy" (puzzles, palentology, dinosaur, manga, smart). There are only two words in the top 25 that refer to romantic relationships: dating and date.

### Type-Token Ratio

The purpose of obtaining a type-token ratio is to explore term usage in and across documents (in our case, across the two different series). We are looking at each unique term (type) and comparing it ot thte total number of terms in the document (tokens). We use the MATTR command in this portion of the script because we want to mitigate the issue of TTR, which is biased when comparing documents with different in the total number of tokens.

```{r a-lexical-ttr}
love_lexdiv <- 
  love_content_tokens %>% 
  textstat_lexdiv(measure = "MATTR", MATTR_window = 100)
```


```{r}
love_tokens_meta <- love_content_tokens %>% docvars() # pull docvars from tokens object
love_lexdiv_meta <- cbind(love_tokens_meta, love_lexdiv) # combine
```

```{r}
love_lexdiv_meta %>%
  ggplot(aes(x = reorder(Series, MATTR), y = MATTR, color = Series)) +
  geom_boxplot(notch = TRUE, show.legend = FALSE) +
  labs(x = "Series")
```

We know that for MATTR (Moving-Average Type-Token Ratio), larger numbers represent a more diverse lexicon and more diversity in words, whereas smaller numbers indicate more repetition. This plot shows that there is more diversity in words in Love on the Spectrum, which is contrary to what we hypothesized; we believed that the individuals on *Love on the Spectrum* used more common and basic words and that there would be more repetition in their transcripts.

### Relative Frequency (Keyness) Measures

The purpose of exploring the relative frequencies of the terms in both TV Series is to compare these terms to one another. One group becomes the target group and the other becomes the reference group. The results will show us which terms occur significantly more often in the target group than they do in the reference group.

```{r relative-frequency-measure}
love_content_keywords <-
  love_content_dfm %>% # dfm
  dfm_trim(min_docfreq = 2) %>% # keep terms appearing in 2 or more episodes
  textstat_keyness(target = love_content_dfm$Series == "Love On The Spectrum")  # compare to Love Is Blind

love_content_keywords %>% 
  textplot_keyness(show_legend = FALSE, n = 25, labelsize = 3)
```

The purpose of this relative frequency graph is to show us the top 25 most frequent words in a target group (Love on the Spectrum) and compares it to the top 25 most frequent words in the reference group (Love is Blind). We see that for Love on the Spectrum, there are a lot of backchannel markers (yeah, um, yes, uh, nice, well, yep, hmm, mmm, and okay) which are probably used as interjections when another is talking.

We also see that "yeah" and "um" occur quite frequently, even more frequent than the most frequent word in Love is Blind, "I". This could also reiterate the idea above.

For Love is Blind, we see more explicits and we also see words that refer to romantic relationships with references to the words husband, connection, engaged, and wife.

### Topic Modeling

```{r}
library(seededlda) # for the Latent Dirichlet Allocation Algorithm
```

```{r topic-modeling-LOTS}
love_lda <- textmodel_lda(love_content_dfm, k = 5)

terms(love_lda, 10) # top 10 terms for each topic
```

There don't seem to any clear topics among the group of words, however we are able to make some connections. For topic two, there are words like yeah, um, yes, good, okay, uh, and well. This goes back to the keyness measure analysis, which showed us how most of the words from Love on the Spectrum were backchannel words. We see this same pattern here. For topic 3, three words stand out: autistic, disabilities, and asperger's. These words instantly draw your attention, so I would say this is the topic for topic 3.

### Sentiment Analysis

We will do a sentiment analysis in order to see if there are certain kinds of emotions that may or may not be associated with each of series. Are the sentiments related to *Love on the Spectrum* more sad and depressing? Do they see their disability as something that holds them back and thus has negative sentiments related to it? Moreover, are there more happy and loving sentiments related to *Love is Blind*? Because they have less difficulty interacting with others and engaging in romantic relationships, are the sentiments in this show more positive?

```{r}
sentiment_dictionary <-
  textdata::lexicon_nrc() %>% # load the nrc lexicon
  as.dictionary() # convert the data frame to a dictionary object
```

```{r sentiment-analysis}
love_sentiment <-
  love_content_tokens %>%
  tokens_lookup(dictionary = sentiment_dictionary) # add sentiment labels
```

We will be using the Word-Emotion Association Lexicon by Mohammad and Turney (2013) in order to see the emotions associated with the words in each series [@mohammad2013].

```{r}
love_sentiment %>%
  dfm() %>% # create dfm
  textstat_frequency(groups = Series) %>% # generate frequency
  mutate(feature_fct = factor(feature, levels = c("positive", "joy", "trust", "anticipation", "surprise", "negative", "frustration", "anger", "fear", "disgust", "sadness"))) %>% # reorder features from positive to negative
  group_by(feature_fct) %>% # grouping parameter
  mutate(prop = round(frequency/sum(frequency), 2)) %>% # create proportions scores
  ggplot(aes(x = feature_fct, y = frequency, fill = group, label = paste0(prop))) + # mappings
  geom_bar(stat = "identity", position = "fill") + # proportion bar plot
  geom_label(position = "fill", vjust = 2) + # add proportion labels
  labs(title = "Sentiment Ratings", fill = "Series", y = "Proportion", x = "Sentiment")

# I don't know how to set this up so that I am looking at the tokens for two different seasons.
```
The sentiment analysis for this project is also something to look at. Firstly, we see that all of these sentiments are expressed more from those in *Love is Blind.* This is very interesting to note because we see them experience and mention a wide range of emotion and they had the ability to recognize and define that feeling. We also see in increase in the sentiment values as the negative sentiments (negative, anger, fear, disgust, and sadness) are mentioned. They seem to express anger and disgust quite frequently, compared to their counterparts on *Love on the Spectrum.* Not as high but still very close to 50%, those on the spectrum showed a deal of positive emotions. Their expression of negative sentiments was lower than their expression of positive sentiments, and even lower than the expression of sentiments for *Love is Blind.* The expressions, both positive and negative, are expressed more widely in one series over another. Is this because neurotypical individuals express their emotions frequently and there is the possibility that *Love is Blind* requires its participants to do this more than natural setting of *Love on the Spectrum.* Moreover, the assumption that negative words may appear in *Love on the Spectrum* due to frustrations with their disability and how it may interfere with their everyday interactions, we don't see this pattern.

## Finalize

### Log

The analysis of the transcripts from Love on the Spectrum and Love is Blind was to provide us with insight on if differences in language occur, especially surrounding the topic of love. Previous research has shown that individuals with Autism Spectrum Disorder often have a difficult time in maintaining relationships, especially romantic ones [@strunz2017].


### Session

<details>

<summary>

View session information

</summary>

```{r, child="_session-info.Rmd"}
```

</details>

## References
