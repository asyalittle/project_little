---
title: "Analyze dataset"
date: "`r Sys.Date()`"
bibliography: references.bib
biblio-style: apalike
---

## About

### Description

The purpose of this script is to compare the two datasets, Love is Blind and Love on the Spectrum and find the differences between the two datasets and thus, the two groups of individuals (those on the Autism spectrum and those who are not). It is important to note that

We will start by doing a keyness contrast analysis by finding the relative frequency of words, then we will do a weighted frequency to determine how frequent a particular word is relative to the entire dialogue from the season. Next, we will look to determine whether or not there is lexical diversity between the two groups. Then, we will do a type-token analysis. Finally we will do topic modeling to show differences amongst different types of people and then the last step we plan to do is a sentiment analysis in order to find if there are different emotions that are expressed in the two different seasons.

It is important to note that though the two shows are about navigating the dating world, we do know that Love on the Spectrum and Love is Blind have different motives. Love on the Spectrum is more about following individuals around as they navigate looking and falling in love, whereas Love is Blind is more of a show based on competition and may be more scripted and less natural.

### Usage

<!-- How to run this script: what input it requires and output produced -->

## Setup

```{r setup}
# Script-specific options or packages

library(tidyverse)           # data manipulation
library(patchwork)           # organize plots
library(janitor)             # cross tabulations
library(tidytext)            # text operations
library(quanteda)            # tokenization and document-frequency matrices
library(quanteda.textstats)  # descriptive text statistics
library(quanteda.textmodels) # topic modeling
library(quanteda.textplots)  # plotting quanteda objects
```

## Run

In our transform stage, we combined the two datasets into one. Let's read in this combined datset.

```{r read-in-dataset, message=FALSE}
love <- read_csv(file = "../data/derived/love.csv")
```

Here, we see this is our new (combined) dataset because it has 10 rows.

```{r}
glimpse(love) # preview dataset 
```

Let's take a look at the data dictionary to make sure we understand what each variable signifies (our variables are very straight forward, but I recommend this step regardless to make sure you know what each variable represents). We will only preview one data dictionary because both contain the same variables: Series, Season, Episode, and Dialogue.

```{r}
read_csv(file = "../data/derived/love_on_the_spectrum/love_on_the_spectrum_curated_data_dictionary.csv") # read data dictionary
```

### Word Frequency Analysis

```{r create-corpus-object}
love_corpus <-
  love %>% # data frame
  corpus(text_field = "Dialogue")

love_corpus %>%
  summary(n = 5) # preview
```

This code chunk allow us to see the type-tokens for each episode in the seasons for Love on the Spectrum and Love is Blind.

```{r create-tokens-object}
love_tokens <-
  love_corpus %>% # corpus object
  tokens(what = "word", # tokenize by word
         remove_punct = TRUE) %>% # remove punctuation
  tokens_tolower() # lowercase tokens


love_tokens %>%
  head(n = 1) # preview one tokenized documents.
```

This shows us each of the tokens, which we organize by word, of each series.

```{r create-document-frequency-matrix}
love_dfm <-
  love_tokens %>%
  dfm() # create dfm

love_dfm %>% # data frequency model for love on the spectrum
  head(n = 5) # preview 5 documents
```

This data frequency model shows the first 10 words of the transcripts from 5 of the 10 episodes (docs). Under each word, it shows its frequency in each of the 5 episodes we are previewing. We see there is a 75.3% sparsity, which indicates there are a number of zeros throughout the dataset for the features. There are 4,122 features total in the 5 documents.

We are now going to look at the top 25 words from the combined dataset.

```{r most-frequent-words}
love_dfm %>%
  textstat_frequency() %>%
  slice_head(n = 10)
```

This list encompasses the top 10 words from each of the documents (episodes). We see that these words are common words that are used in everyday language use.

We are now going to use this data frame to plot the frequencies of the terms in descending order for the dataset (the top 25 words).

```{=html}
<!-- JF: 
Note: your title needs updating as this now is for both series
-->
```


```{r dfm-ggplots}
love_dfm %>%
  textstat_frequency() %>%
  slice_head(n = 25) %>%
  ggplot(aes(x = reorder(feature, frequency), y = frequency)) + geom_col() + coord_flip() + labs(x = "Words", y = "Raw Frequency", title = "Top 25 for Love on the Spectrum")
```

The raw frequency of each word in each dataset is affected by the total number of words in each series dataset. In order to make a term-series comparison, we will now use the dfm_weight() function which will determine the Term (weighted) frequency of the words by determining how frequent a term in an episode is to the rest of the episode.

```{r weighted-term-frequency}
love_dfm %>% # dfm
  dfm_weight(scheme = "prop") %>% # weigh by term frequency
  textstat_frequency() %>%
  group_by(group) %>% #grouping parameters
  slice_max(frequency, n = 15) %>% # extract top features
  ungroup() %>% # remove grouping parameters
  ggplot(aes(x = frequency, y = reorder_within(feature, frequency, group), fill = group)) +
  geom_col (show.legend = FALSE) + # barplot
  scale_y_reordered() + # clean up y-axis labels (features)
  labs(x = "Term Frequency", y = NULL) # labels
```

We see the use of the words "I" and "You" are used very frequently, much more than the rest of the words.

The issue we now have is that the words in the term frequency matrix is that the top 15 words appear to be terms that are the bread and butter across language-- they are extremely common. In order to find the words that distinguish (if they do) one series from another, we must apply the Term Frequency-Inverse Document Frequency to put the most common words at the bottom of the chart because they are most frequent across all the documents, and put the next set of words that aren't as common as the most frequent.

```{r tf-idf}
love_dfm%>%
  dfm_trim(min_docfreq = 2) %>% # keep terms appearing in 2 or more episodes
  dfm_tfidf(scheme_tf = "prop") %>% # weigh by tf-idf
  textstat_frequency(force = TRUE) %>% # get frequency statistics
  group_by(group) %>% #grouping parameters
  slice_max(frequency, n = 15) %>% # extract top features
  ungroup() %>% # remove grouping parameters
  ggplot(aes(x = frequency, y = reorder_within(feature, frequency, group), fill = group)) +
  geom_col (show.legend = FALSE) + # barplot
  scale_y_reordered() + # clean up y-axis labels (features)
  labs(x = "Term Frequency", y = NULL) # labels
```

After minimizing the effect of common words, we are now able to see the "meat" of the data and which words are used most frequently across the episodes. We see there are a lot of names that appear. We see no words relating to romantic relationships, but we do see explicits as well as references to autism and the spectrum.

Since the shows are dating shows and both reality shows in a sense, it makes sense that names are very common throughout the shows. However, this is distracting and doesn't allow us to really analyze whether we see patterns in the most frequent words.

Let's look at loading in and using the `babynames` dataset in order to eliminate the names we see listed above and look at the true content of the dataset.

```{r a-love-remove-names}
first_names <- 
  babynames::babynames %>% # babynames data frame
  distinct(name) %>% # get unique names
  pull(name) %>% # pull names into character vector
  str_to_lower() # lowercase all names

love_content_tokens <- 
  love_tokens %>% # original tokens object
  tokens_remove(first_names) # remove names in `first_names`

love_content_dfm <- 
  love_content_tokens %>% 
  dfm()

love_content_dfm %>% 
  dfm_tfidf() %>% 
  dfm_trim(min_docfreq = 2) %>% 
  textstat_frequency(groups = Series, force = TRUE) %>% 
  group_by(group) %>% 
  slice_max(frequency, n = 25) %>% 
  ungroup() %>% 
  ggplot(aes(x = frequency, y = reorder_within(feature, frequency, group), fill = group)) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  facet_wrap(~group, scales = "free_y") +
  labs(x = "Raw Frequency", y = NULL)
```

After eliminating all the names that took appeared in the top 15 word frequency, we are finally able to look at the meat of the dataset. We are also able to compare the raw frequency of the top 25 words of both Series. We see for the Love is Blind series, we see explicits are said quite frequently, but we also see words that reference love and romantic relationships like engaged, husband, wife, propose, fianc√©, wedded, and kids.

For the Love on the Spectrum series, there are a lot of references to Austism (autistic, autism, spectrum, asperger's, disability) and words that are "nerdy" (puzzles, palentology, dinosaur, manga, smart). There are only two words in the top 25 that refer to romantic relationships: dating and date.

### Type-Token Ratio

The purpose of obtaining a type-token ratio is to explore term usage in and across documents (in our case, across the two different series). We are looking at each unique term (type) and comparing it ot thte total number of terms in the document (tokens). We use the MATTR command in this portion of the script because we want to mitigate the issue of TTR, which is biased when comparing documents with different in the total number of tokens.

```{r a-lexical-ttr}
love_lexdiv <- 
  love_content_tokens %>% 
  textstat_lexdiv(measure = "MATTR", MATTR_window = 100)
```

```{=html}
<!-- JF:

You were trying to cbind the love_corpus object with the love_lexdiv object --and you need to combine the metadata (docvars) from the love_content_tokens object.
-->
```
```{r}
love_tokens_meta <- love_content_tokens %>% docvars() # pull docvars from tokens object
love_lexdiv_meta <- cbind(love_tokens_meta, love_lexdiv) # combine
```

```{r}
love_lexdiv_meta %>%
  ggplot(aes(x = reorder(Series, MATTR), y = MATTR, color = Series)) +
  geom_boxplot(notch = TRUE, show.legend = FALSE) +
  labs(x = "Series")
```

We know that for MATTR (Moving-Average Type-Token Ratio), larger numbers represent a more diverse lexicon and more diversity in words, whereas smaller numbers indicate more repetition. This plot shows that there is more diversity in words in Love on the Spectrum, which is contrary to what we hypothesized; we believed that the individuals on *Love on the Spedtrum* used more common and basic words and that there would be more repetition in their transcripts.

### Relative Frequency (Keyness) Measures

The purpose of exploring the relative frequencies of the terms in both TV Series is to compare these terms to one another. One group becomes the target group and the other becomes the reference group. The results will show us which terms occur significantly more often in the target group than they do in the reference group.

```{r relative-frequency-measure}
love_content_keywords <-
  love_content_dfm %>% # dfm
  dfm_trim(min_docfreq = 2) %>% # keep terms appearing in 2 or more episodes
  textstat_keyness(target = love_content_dfm$Series == "Love On The Spectrum")  # compare to Love Is Blind

love_content_keywords %>% 
  textplot_keyness(show_legend = FALSE, n = 25, labelsize = 3)
```

The purpose of this relative frequency graph is to show us the top 25 most frequent words in a target group (Love on the Spectrum) and compares it to the top 25 most frequent words in the reference group (Love is Blind). We see that for Love on the Spectrum, there are a lot of backchannel markers (yeah, um, yes, uh, nice, well, yep, hmm, mmm, and okay) which are probably used as interjections when another is talking.

We also see that "yeah" and "um" occur quite frequently, even more frequent than the most frequent word in Love is Blind, "I". This could also reiterate the idea above.

For Love is Blind, we see more explicits and we also see words that refer to romantic relationships with references to the words husband, connection, engaged, and wife.

### Topic Modeling

```{r}
library(seededlda) # for the Latent Dirichlet Allocation Algorithm
```

```{r topic-modeling-LOTS}
love_lda <- textmodel_lda(love_content_dfm, k = 5)

terms(love_lda, 10) # top 10 terms for each topic
```

There don't seem to any clear topics among the group of words, however we are able to make some connections. For topic two, there are words like yeah, um, yes, good, okay, uh, and well. This goes back to the keyness measure analysis, which showed us how most of the words from Love on the Spectrum were backchannel words. We see this same pattern here. For topic 3, three words stand out: autistic, disabilities, and asperger's. These words instantly draw your attention, so I would say this is the topic for topic 3.

### Sentiment Analysis

We will do a sentiment analysis in order to see if there are certain kinds of emotions that may or may not be associated with each of series. Are the sentiments related to *Love on the Spectrum* more sad and depressing? Do they see their disability as something that holds them back and thus has negative sentiments related to it? Moreover, are there more happy and loving sentiments related to *Love is Blind*? Because they have less difficulty interacting with others and engaging in romantic relationships, are the sentiments in this show more positive?

```{r}
sentiment_dictionary <-
  textdata::lexicon_nrc() %>% # load the nrc lexicon
  as.dictionary() # convert the data frame to a dictionary object
```

```{r sentiment-analysis}
love_sentiment <-
  love_content_tokens %>%
  tokens_lookup(dictionary = sentiment_dictionary) # add sentiment labels
```

We will be using the Word-Emotion Association Lexicon by Mohammad and Turney (2013) in order to see the emotions associated with the words in each series \[@mohammad2013\].

## Finalize

### Log

The analysis of the transcripts from Love on the Spectrum and Love is Blind was to provide us with insight on if differences in language occur, especially surrounding the topic of love. Previous research has shown that individuals with Autism Spectrum Disorder often have a difficult time in maintaining relationships, especially romantic ones [@strunz2017].

Our purpose is to investigate whether or not we see differences in word frequency (due to a common characteristic of autism being *echolalia*, the repetition of the speech of others) [@sterponi2014]. Due to this common characteristic, we were hoping to find patterns in the Love on the Spectrum data to validate this hypothesis, however it does not appear that words indicative of repetition (questioning why if someone asks why, etc.) occur in this data.

We also wished to investigate which words were more common in each dataset. We found that word names seemed to appear most often when controlling for common words expressed in everyday language use and for Love is Blind, we found words indicative of relationships and romance. Next, we looked at sentiments and topics surrounding each of the TV Series and we didn't find anything indicative that romance was more of a topic in Love is Blind than in Love on the Spectrum. Instead, the only theme that seemed to appear in the Love on the Spectrum topics were the use of common words. We also see that their word choice across topics is very basic, which is also a characteristic of autism; their word choice compared to typically-developed adults is simpler. For the Love is Blind topics, we found the common words topic like we did in Love on the Spectrum, but we also saw the topic of home, which either shows that individuals talked about their lives growing up, or talked about their lives together. Again, however, we did not find that romance was a topic that was more discussed in Love is Blind than in Love on the Spectrum.

We did further investigation on whether or not we see echolalia in individuals on the spectrum by focusing on the Type-Token Ratio.

Finally, we wanted to investigate whether or not individuals on the spectrum had a smaller vocabulary than those not on the spectrum, and their use of very common words like you, I, yeah, like, etc.

\*\*\* for some reason, my citations will not show up when I put the \@ symbol. Went in to make sure zotero key was there, it was. Cleared out and re-entered, reapplied, and hit ok. Still did not work.

### Session

<details>

<summary>

View session information

</summary>

```{r, child="_session-info.Rmd"}
```

</details>

## References
